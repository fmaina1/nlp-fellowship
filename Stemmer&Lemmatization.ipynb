{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmaina1/nlp-fellowship/blob/main/Stemmer%26Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "*A stemming algorithm, a procedure to reduce all words with the same\n",
        "stem to a common form, is useful in many areas of computational linguistics and information-retrieval work.* ~ Lovin,1968\n",
        "\n",
        "Examples of Stemmers include:\n",
        "\n",
        "\n",
        "1.   PorterStemmer\n",
        "2.   SnowballStemmer\n",
        "3. LancasterStemmer\n",
        "4. RegexStemmer\n",
        "\n"
      ],
      "metadata": {
        "id": "in_28geDz1Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "OYQ40jZx-Awx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PorterStemmer\n",
        "Designed and built by Martin Porter in 1980\n",
        "\n",
        "Takes Five steps each with its own mapping rules. Easy and fast\n"
      ],
      "metadata": {
        "id": "pP3w12Sy0QDd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edofg-4JoDXD"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer,LancasterStemmer,RegexpStemmer,WordNetLemmatizer\n",
        "words = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"generate\",\"generates\",\"generating\",\"general\",\"generally\",\"generic\",\"generically\",\"generous\",\"generously\",\"went\",\"ate\"]\n",
        "Porter = PorterStemmer()\n",
        "\n",
        "for word in words:\n",
        "    print(word,\"--->\",Porter.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_words = [\"walk\",\"walking\",\"walked\",\"walks\"]\n",
        "for word in new_words:\n",
        "    print(word,\"--->\",Porter.stem(word))"
      ],
      "metadata": {
        "id": "oupMbnhPXUA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SnowballStemmer/Porter2Stemmer\n",
        "Designed and built by Martin Porter\n",
        "Advancement of PorterStemmer\n",
        "\n",
        "Faster and more precise than Porter Stemmer"
      ],
      "metadata": {
        "id": "U964E_O53SMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snowball = SnowballStemmer(language='english')\n",
        "\n",
        "for word in words:\n",
        "    print(word,\"--->\",snowball.stem(word))"
      ],
      "metadata": {
        "id": "9_XdGLfFoSr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LancasterStemmer\n",
        "Simpler\n",
        "\n",
        "Results to over stemming of words, which leads to meaningless words"
      ],
      "metadata": {
        "id": "VNRbB3oD5TIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lancaster = LancasterStemmer()\n",
        "\n",
        "for word in words:\n",
        "    print(word,\"--->\",lancaster.stem(word))"
      ],
      "metadata": {
        "id": "KXm57JXP27mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RegexStemmer\n",
        "Uses regex\n",
        "\n",
        "Substring matching the regex will be discarded\n",
        "\n",
        "Worst performer"
      ],
      "metadata": {
        "id": "5B9i26p95cKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regex = RegexpStemmer('ing$|s$|e$|able$|lly$|ate$', min=3)\n",
        "\n",
        "for word in words:\n",
        "    print(word,\"--->\",regex.stem(word))"
      ],
      "metadata": {
        "id": "kGwXP_lk5P6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmantizing"
      ],
      "metadata": {
        "id": "CQvja1ZC9pf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet = WordNetLemmatizer()\n",
        "lemm_word = ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
        "for word in lemm_word:\n",
        "    print(word,\"--->\",wordnet.lemmatize(word))"
      ],
      "metadata": {
        "id": "9gSWwpmz8ElG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize"
      ],
      "metadata": {
        "id": "2aGUc4uhGh-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import  pos_tag\n",
        "text = '''President Paul Kagame has said that deliberate efforts are needed to forge private-public partnerships to bridge internet usage gaps. He was speaking during the inauguration of the Mobile World Congress 2022 which convened more than 2000 people representing 99 countries, on October 25.\n",
        "Global mobile operators, device manufacturers, technology providers, vendors, content owners, and policymakers are in Kigali to identify gaps and discuss effective measures needed to drive digital transformation in Africa. To address the usage gap –the number of people who can’t use mobile internet services while living in an area covered by broadband networks –Kagame said that neither the private nor the public sector has all that is required to cover the gap, hence, the need for partnerships. '''\n",
        "\n",
        "tokens = list(tokenize(text))\n",
        "tokens"
      ],
      "metadata": {
        "id": "k7AVlGqB93oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_list = pos_tag(tokens)"
      ],
      "metadata": {
        "id": "Yl6KV33OF8zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matched_tags = {'NNP':'n',\"VBP\":'v'}\n",
        "processed_tag = []\n",
        "for token, tag in pos_list:\n",
        "  token = wordnet.lemmatize(token,matched_tags[tag])\n",
        "  processed_tag.append(token)\n",
        "  #print(token,'-------------',tag)"
      ],
      "metadata": {
        "id": "xj3N2oK7aFZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(wordnet.lemmatize('countries'))\n",
        "#pos_tag(['best'])\n",
        "print(wordnet.lemmatize('better','a'))"
      ],
      "metadata": {
        "id": "4eVyoDAUG3vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords\n",
        "Common simple words that add little value\n",
        "\n",
        "The goal is to reduce the size of the matrix as much as possible, therefore removing common words that do not add value makes sense. An example is I, a, an\n"
      ],
      "metadata": {
        "id": "O5hRNTOBIPMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "sw = stopwords.words('english')\n",
        "print(sw)"
      ],
      "metadata": {
        "id": "pXgd4po-HVpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)\n",
        "\n",
        "tokens_no_sw = []\n",
        "for token in tokens:\n",
        "  if token not in sw:\n",
        "    tokens_no_sw.append(token)\n",
        "\n",
        "print(len(tokens_no_sw))"
      ],
      "metadata": {
        "id": "9W8cv2tuRrqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In-class practicls\n",
        "1. How many stop words are in NLTK, Spacy,Gensim. Compare them an select one\n",
        "2. Lemmantize the above text using a for loop\n",
        "3. Compare the Stemmers, get the best and compare in with Lemmantizer. \n",
        "4. Remove stop words from the text "
      ],
      "metadata": {
        "id": "s8DBaIAAJt1C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "Create a function that takes the tokens, normalize the tokens and remove the stop words  "
      ],
      "metadata": {
        "id": "7B9p52D9KV_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def stemming_lem_sw (tokens):\n",
        "  new_tokens = []\n",
        "  for token in tokens:\n",
        "    token = snowball.stem(token)\n",
        "    if token not in sw:\n",
        "      new_tokens.append(token)\n",
        "\n",
        "  return new_tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "zTd5zyg0I8wQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}